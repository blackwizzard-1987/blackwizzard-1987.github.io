---
layout:     post
title:      TiCDC同步功能测试
subtitle:  	
date:       2021-05-25
author:     RC
header-img: 
catalog: true
tags:
    - TiDB
    - TiCDC
    - 增量同步
---

## 1.  测试目的

- TiCDC的配置和性能,延迟，对现有tidb集群的影响
- TiCDC的监控，维护，可靠性(是否会数据丢失)
- 是否可能等价2个机房的数据做一个备份(高可用)
- 额外的一套tidb的监控等工作
- 如果数据对称，能否实现上海机房的备份功能，和添加tiflash后读写分离的功能


## 2. TiCDC原理简介和DM对比

### 2.1 TiCDC原理简介

TiDB 4.0 引入的新组件 Change Data Capture（CDC）是用来识别、捕捉和交付 TiDB/TiKV 数据变更的工具系统。在 TiDB 生态链上，CDC 作为 TiDB 的数据出口有着非常重要的地位，其作用包括： **构建**  **TiDB**  **主从和灾备系统；链接**  **TiDB**  **和其它异构数据库；提供开放数据协议，支持把数据发布到第三方系统** 。

它通过识别、捕捉和输出 TiDB/TiKV 集群上数据变更，所有通过MVCC层的change log都会被记录，排序，最终输出到下游。

组件：

**TiKV** ：输出 KV 变更日志（KV Change Logs），TiKV 负责拼装 KV 变更日志，并输出到 TiCDC 集群。

**Capture** ：TiCDC 的运行进程。一个 TiCDC 集群通常由多个 capture 节点组成，每个节点负责拉取一部分 KV 变更日志，排序后输出到下游组件。一个TiCDC 集群里有两种 capture 角色：owner 和 processor。

一个 TiCDC 集群有且仅有一个 owner，它负责集群内部调度。若 owner 出现异常，则其它 capture 节点会自发选举出新的 owner。

processor 在实现上是 capture 内部的逻辑线程。一个 capture 节点可以运行多个 processor，每个 processor 负责同步若干个表的数据变更。processor 所在的 capture 节点若出现异常，则 TiCDC 集群会把同步任务重新分配给其它 capture 节点。

**Sink** ：TiCDC 内部负责将已经排序的数据变更同步到下游的组件。TiCDC 支持同步数据变更到多种下游组件，包括 **TiDB** ， **MySQL** ，Kafka，Pulsar等

### 2.2 与另一个同步工具DM的对比

| 工具名 | 优点 | 缺点 |
| --- | --- | --- |
| DM (TiDB Data Migration) | 1. 简单可控 2. 占用资源少 3. 运维成本低 | ①仅支持异构数据库(MySQL体系)到TiDB的数据迁移 ②同步依赖binlog，无法大规模应用 ③单点故障|
| TiCDC（TiDB Change Data Capture） | ① 支持将TiDB数据输出到各个下游，包括tidb，mysql和kafka ② 可以应对大量数据变更的场景，且同步延迟低 ③可用于增量备份，镜像 ④高可用 | ①需要额外的大量资源支撑processor对change log的排序 ②同步的表有限制(必须含有主键/非空唯一索引) ③Region 无法进入静默状态 ④不支持大事务(超过5G)


## 3. 测试准备工作

```html
--通过tiup部署TiCDC节点
$ vim cdc_scale_out.yaml
cdc_servers: 
- host: xx.31.xx.29 
- host: xx.31.xx.32
$ tiup cluster scale-out tidb cdc_scale_out.yaml 
```

```html
--CLI界面查看CDC状态
$ tiup ctl cdc -i --pd http://xx.31.xx.32:2379
$ capture list
$ changefeed list
$ processor list 
```

TiCDC的同步任务建议使用tidb-lighting辅助的方式，进行增量同步，比较方便操作

如果想使用TiCDC完成全量+增量同步，进行datetime到ts的转换(较复杂)，指定创建同步任务时的start-ts

## 4. 同步功能测试

所有同步任务创建时的start-ts都必须比上游tidb的GC时间大，默认为当前时间

### 4.1 初始化同步任务时(上游已有数据)

```html
--dumpling导出需要同步的表/库(全备)
$ dumpling -h xx.33.xx.60 -P 4000 -u DBA_RC -t 8 -r 50000 -T CRM.OrderTag -p"xxx" -o ./CRM_OrderTag
…

--tidb-lighting将全备导入到下游(模式=local)
$ vim tidb-lighting.toml
[lightning]
# 转换数据的并发数，默认为逻辑 CPU 数量，不需要配置。
# 混合部署的情况下可以配置为逻辑 CPU 的 75% 大小。
# region-concurrency =

# 日志
level = "info"
file = "tidb-lightning.log"

[tikv-importer]
# backend 设置为 local 模式
backend = "local"
# backend = "tidb"
# on-duplicate = "replace"
# 设置本地临时存储路径
sorted-kv-dir = "/restore/sorted-kv-dir"

[mydumper]
# Mydumper 源数据目录。
data-source-dir = "/restore/EhaiBizLog_ChannelPurchaseAddedServicesLogInfo"

[tidb]
# 目标集群的信息。tidb-server 的监听地址，填一个即可。
host = "xx.31.xx.30"
port = 4000
user = "root"
password = ""
# 表架构信息在从 TiDB 的“状态端口”获取。
status-port = 10080
# pd-server 的地址，填一个即可
pd-addr = "xx.31.xx.30:2379"

$ tidb-lightning -config tidb-lightning.toml

--获取同步任务的start-ts(增量同步开始位置)
$ cat ./CRM_OrderTag/metadata
SHOW MASTER STATUS:
		Log: tidb-binlog
		Pos: 424113326306099210

--创建TiCDC同步任务
$ vim changefeed.toml
# 指定配置文件中涉及的库名、表名是否为大小写敏感
# 该配置会同时影响 filter 和 sink 相关配置，默认为 true
case-sensitive = true

# 是否输出 old value，从 v4.0.5 开始支持，从 v5.0 开始默认为 true
enable-old-value = false

[filter]
# 忽略指定 start_ts 的事务
# ignore-txn-start-ts = [1, 2]

# 过滤器规则
# 过滤规则语法：https://docs.pingcap.com/zh/tidb/stable/table-filter#表库过滤语法
rules = ['CRM.*']

[mounter]
# mounter 线程数，用于解码 TiKV 输出的数据
worker-num = 16

[cyclic-replication]
# 是否开启环形同步
enable = false
# 当前 TiCDC 的复制 ID
replica-id = 1
# 需要过滤掉的同步 ID
filter-replica-ids = [2,3]
# 是否同步 DDL
sync-ddl = true

$ tiup cdc cli changefeed create --pd=http://xx.31.xx.30:2379 --sink-uri "mysql://root:@xx.29.xx.30:4000/" --changefeed-id="simple-replication-task" --start-ts xxxx --config changfeed.toml

--查看同步任务状态
$ tiup ctl cdc -i --pd http://xx.31.xx.32:2379/
$ changefeed list
$ changefeed query --changefeed-id=simple-replication-task
$ vim /tidb-deploy/cdc-8300/log/cdc.log
```

>> 注意：TiCDC的DML同步中的insert和update操作在下游回放时会被替换为replace操作，即如果下游delete了某条数据，而上游update了这条数据，那么下游会insert这条数据 


### 4.2 初始化同步任务时(上游也没有数据)

```html
--dumpling导出需要同步的表/库(全备)
$ dumpling -h xx.33.xx.60 -P 4000 -u DBA_RC -t 8 -r 50000 -T CRM.User_Tag_43 -p"" -o ./CRM_ User_Tag_43

--创建TiCDC同步任务
配置文件同4.1

--下游tidb创建需要同步的库
$ create database CRM;

--tidb-lighting将全备导入到上游(模式=tidb)
$ vim  tidb-lightning.toml
[tikv-importer]
backend = "tidb"
on-duplicate = "replace"
$ tidb-lightning -config tidb-lightning.toml

--查看同步任务状态
$ changefeed query --changefeed-id=simple-replication-task
$ vim /tidb-deploy/cdc-8300/log/cdc.log
```

>> 注意：Tidb-lighting在tidb模式下会将导入数据编码为insert语句，因此能被CDC捕获变更日志；如果使用其他两种模式，将会因为数据被编码为键值对(sst文件)直接  Ingest 到tikv底层，不会经过MVCC层，因此不会产生change log，不会同步到下游 

### 4.3 在已有同步任务上添加同步对象/修改配置时

```html
--下游tidb创建需要同步的库
$ create database xxx;

--暂停需要修改配置的CDC任务
$ tiup ctl cdc -i --pd http://xx.31.xx.32:2379
$ changefeed pause -c simple-replication-task

--修改CDC配置文件，添加新的同步库
$ vim changefeed.toml
rules = ['CRM.*', 'test1.*', 'CRM_Tag.*', 'EhaiBizLog.*']

--更新CDC任务
$ changefeed update -c simple-replication-task --sink-uri="mysql://xx.29.xx.30:4000/" --config=changefeed.toml
确认更改内容后，Y

--恢复同步任务
$ changefeed resume --changefeed-id simple-replication-task

--查看同步任务状态
$ changefeed query --changefeed-id=simple-replication-task
$ vim /tidb-deploy/cdc-8300/log/cdc.log

--删除同步任务
$ changefeed remove --changefeed-id simple-replication-task --force
```

### 4.4 同步任务延迟监控

图略

可以看到processor上的cdc进程在大量数据同步期间占用CPU资源和内存资源很高，建议将CDC的2个节点单独部署，并且其中一个(指定为processor)使用较多CPU核和内存

## 5. 结论

使用TiCDC同步广州tidb数据到上海tidb，达到镜像备份的目的，且可以设置两个机房的读写分离功能，最终预计效果如下：

![1](https://i.postimg.cc/901n72KS/1.png)

但其可靠性(是否能在故障后中断恢复)，同步数据完整性，压力测试，以及对现有tidb群集的影响未知，需要后续第二个DC搭建后进行测试观察

## 6. 参考

[https://book.tidb.io/session2/chapter2/cdc-internal.html](https://book.tidb.io/session2/chapter2/cdc-internal.html)

[https://docs.pingcap.com/zh/tidb/stable/manage-ticdc](https://docs.pingcap.com/zh/tidb/stable/manage-ticdc)

[https://asktug.com/t/topic/70170](https://asktug.com/t/topic/70170)