---
layout:     post
title:      数据分析平台小结
subtitle:
date:       2023-11-14
author:     RC
header-img:
catalog: true
tags:
    - 数据仓库
    - Flink
    - 数据分析
    - 大数据
    - 流计算
---

### 项目背景

本文是针对成都市“蓉易办”数据分析管理系统（以下简称数据分析平台）中技术架构、数据仓库建设和数据分析工作的内容的小结。

数据分析平台的主要数据来源为“天府蓉易办”平台和省一体化平台的政务数据。

“天府蓉易办”平台是成都按照国家和四川省关于一体化在线政务服务平台的相关要求，在吸收借鉴先进城市优秀经验基础上，打造的智能化政务服务一网通办平台，让企业和群众办事 “上一张网，最多跑一次、 一次能办成” 。同时，也是成都市优化营商环境的重要品牌。

> 本文编写时间为2023年11月，蓉易办1期项目已于2023年10月验收完毕

数据分析平台建设目的主要有：

- 解决数据实时同步问题，将业务数据及时同步给数据分析管理平台使用
	 
- 建立蓉易办数据仓库，实现数据来源、数据治理、数据共享统一管理
	 
- 实现办件、排对叫号、好差评、事项、行政区划和部门等业务、指标数据实时同步，实时计算和离线计算
	 
本人根据公司安排参与了数据分析平台自2023年1月至今的数仓建设、数据分析、数据共享、故障处理等工作，因工作中首次涉及到了Flink CDC、TDSQL等组件的使用，遇到了一些问题并采取了解决办法，所以编写了本文做一个阶段性的技术总结。

### 数据分析平台技术架构

数据分析平台整体的技术架构如下：

![1](https://i.postimg.cc/kXxWHx4M/image.jpg)

可以看到比较的轻型且容易理解，没有任何中间件，是一个典型的用Flink+TDSQL实现的流批一体、实时计算的OLAP架构。

总的流程为：

业务系统、省一体化平台等数据源将数据同步到TDSQL形成的前置库， 数据分析平台通过Flink CDC实时捕获数据变化，经过实时计算同步、加工数据到数据仓库各层（ODS, DWD, DIM, ADS层）。实时计算处理实时业务需求，离线计算处理非实时业务需求（如业务统计指标计算），最终提供数据到蓉易办数据管理分析平台用于统计分析、支撑大屏数据展示、支持其他平台的数据共享交换等。

> 因TDSQL本身是一个分布式的类MySQL数据库，因此提供了binlog，这使得Flink CDC不需要任何额外的组件就可以通过binlog捕获所有数据的状态变化和更新，达到流计算的要求。

> 但同时，我们应该清楚的认识到，作为实时计算的代价，基于Flink CDC的流计算任务根据计算的复杂度和表的大小所耗费的内存可能极大

### Flink CDC简介

#### 数据接入
	
数据仓库的数据来源主要是日志和数据库，其中数据库接入比较复杂，相关的工具包括Canal，Debezium，Maxwell。Flink通过CDC format与这些同步工具做了很好的集成，可以**直接消费这些同步工具产生的数据**：

![5](https://i.postimg.cc/VkP32Qm3/4-B1903-F2-C896-4251-BE64-C486-AC7-E82-B7.png)

同时Flink还推出了**原生的CDC connector，直连数据库**，降低接入门槛，简化数据同步流程。

在最初的常见设计中，通过 Debezium或者Canal去实时采集MySQL数据库的 binlog，并将行级的变更事件同步到Kafka中供Flink分析处理。**在Flink推出CDC format之前，用户要去消费这种数据会非常麻烦**，用户需要了解CDC工具的数据格式，将before，after等字段都声明出来，然后用ROW_NUMBER做个去重，来保证实时保留最后一行的语义。但这样使用**成本很高，而且也不支持 DELETE 事件**。

现在Flink支持了CDC format，我们在源CDC表的建表语句中的with参数中可以直接指定format = ‘debezium-json’，然后schema部分只需要填数据库中表的schema即可。Flink能自动识别Debezium 的 INSERT/UPDATE/DELETE事件，并转成Flink内部的INSERT/UPDATE/DELETE消息。**我们可以在该表上直接做聚合、join等操作，就跟操作一个MySQL实时物化视图一样，非常方便**：

![2](https://i.postimg.cc/pTBfVjFf/592e206c4baf41bca2c7086a842780c5.png)

**"MySQL实时物化视图"**

FlinkSQL定义的动态表，动态表和流的概念是对等的。参照下图，流可以转换成动态表，动态表也可以转换成流：

![3](https://i.postimg.cc/HLVQ3bwk/B5-C79-F14-6836-4f04-B9-FA-957-E141-BAD8-D.png)

在Flink中，数据在从一个算子流向另外一个算子时都是以Changelog Stream的形式，任意时刻的Changelog Stream可以翻译为一个表，也可以翻译为一个流。联想下MySQL中的表和binlog日志，就会发现：MySQL数据库的一张表所有的变更都记录在binlog日志中，如果一直对表进行更新，binlog日志流也一直会追加，数据库中的表就相当于binlog日志流在某个时刻点物化的结果；日志流就是将表的变更数据持续捕获的结果。这说明Flink的Dynamic Table是可以**非常自然地表示一张不断变化的MySQL数据库表**。

> 项目上的Flink版本为1.13.2，mysql-cdc版本为2.2.0

#### Flink CDC 同步原理

新版本中，Flink内部原生支持了CDC的语义，所以可以很自然地直接去读取MySQL的binlog数据并转成Flink内部的变更消息。

利用MySQL CDC connector，你只需要在with参数中指定**connector=mysql-cdc**，然后select这张表就能实时读取MySQL中的全量+CDC增量数据，无需部署其他组件和服务。Flink中定义的这张表可以理解成是MySQL的实时物化视图（如上文所述），所以在这张表上的聚合、join等结果，跟实时在MySQL中运行出来的结果是一致的。

相比于之前设计中的Debezium，Canal的架构，CDC connector在使用上更加简单易用了，不用再去学习和维护额外组件，数据不需要经过Kafka落地，减少了端到端延迟。而且**支持先读取全量数据，并无缝切换到CDC增量读取上，也就是我们说的是流批一体，流批融合的架构**。

MySQL CDC connector非常受用户的欢迎，尤其是**结合OLAP引擎（Doris、ClickHouse、TiDB等），可以快速构建实时OLAP架构**。实时OLAP架构的一个特点就是将数据库数据同步到OLAP中做即席查询，这样就**无需离线数仓**了。

**为什么是流批一体？**

> 先看看之前设计中的流计算方式：用 datax 做个全量同步，然后用 canal 同步实时增量到 Kafka，然后从 Kafka 同步到 OLAP，这种架构比较复杂，链路也很长。

以Flink+ClickHouse为例，**只需要在 Flink 中定义一个MySQL-CDC source，一个ClickHouse sink，然后提交一个insert into query的Flink SQL，就完成了从MySQL到ClickHouse的实时同步工作**，非常方便。而且，ClickHouse有一个痛点就是join比较慢，所以一般我们会把MySQ 数据打成一张大的明细宽表数据，再写入 ClickHouse。这个**打宽的动作在 Flink 中一个join操作就完成了**：

![6](https://i.postimg.cc/DwzY92Rp/B70-D876-B-0931-4e72-9-BE9-CF5-E155-B19-E3.png)

> 在Flink提供MySQL CDC connector之前，要在全量+增量的实时同步过程中做 join 是非常麻烦的

> 当然，根据join的表大小和复杂程度，需要的join内存也是很大的

本项目中，只是将source和sink换成了TDSQL source、TDSQL Sink(TDSQL有完整的Binlog)，加上insert into query的Flink SQL，就完成了Flink+TDSQL快速构建实时OLAP架构的数据仓库。

> 这里的source表'connector'='mysql-cdc'，sink表'connector'='jdbc'（因为写入的OLAP是TDSQL）

> 对于一些无法支持原生CDC的数据源，只能通过'connector'='kafka'，'format'='debezium-json'等接入，但是sink的connector支持很多类型，比如绝大多数的OLAP数据库（Doris、ClickHouse、TiDB、TDSQL等），以及Hive、HBase、Iceberg、upsert-kafka

![4](https://i.postimg.cc/htwJD6mK/40-C866-FF-E05-E-4622-A83-E-C980-CDD27346.png)

该例子是通过Flink CDC去同步数据库数据并写入到TiDB，用户直接使用Flink SQL创建了产品和订单的MySQL-CDC表，然后对数据流进行join加工，加工后直接写入到下游数据库。**通过一个Flink SQL作业就完成了CDC的数据分析，加工和同步**。

并且这个同步和加工动作是一个**纯SQL作业**，这意味着只要会SQL的BI、业务线同学都可以完成此类工作。与此同时，用户也可以利用Flink SQL提供的丰富语法进行数据清洗、分析、聚合。

总结，利用Flink CDC快速构建实时OLAP架构的优势有：

- **简单易用，纯SQL作业**

- **没有额外中间件，数据不落地，减少了存储成本和数据延迟**

- **支持全量读取+CDC增量读取，完全的无缝切换，流批一体**

- **支持Exactly Once读取和处理，数据一致**

- **明细宽表直接入OLAP，统一流批存储，无需离线数据仓**

- **链路更短更直接，应用层接入成本降低，灵活应对需求变更**


#### 常见的打宽操作

##### Regular Join

Regular Join即为双流join，Regular Join的特点是，**任意一侧流的数据更新都会触发结果的更新**（都会重新计算一次），其语法与传统的批处理SQL一致。

Regular join涉及到两个或多个流的联接，因此需要**在内存中存储与这些流相关的数据以便进行联接操作，通过state来存储双流已经到达的数据**，state默认永久保留（实际上记录了两个join流的数据的全部状态）。因此为了管理这些状态并防止内存溢出，Flink引入了TTL机制。

TTL定义了与Regular join操作相关联的状态在内存中可以被保留的最长时间。一旦状态数据超过了这个时间限制，Flink就会自动清理这些数据，释放内存空间。**TTL的值可以根据具体的业务需求和系统的性能要求进行设置**。较大的TTL值意味着状态数据可以在内存中保留更长的时间，从而可能提高查询的准确性和效率，但也可能增加内存使用的压力。相反，较小的TTL值可以减少内存使用，但可能降低查询的准确性或增加延迟。

> 如果两个流之间的数据到达速率严重不平衡，或者存在大量的重复数据，那么即使设置了TTL也可能无法完全避免内存溢出的问题。

> 在Flink中，TTL可以应用于不同类型的state，包括ValueState、ListState、MapState等。对于Regular join操作，通常需要根据具体的实现和需求来确定应该为哪种类型的state设置TTL。

Regular Join的特点总结如下：

- 任意一流都会触发对结果的更新

- 语法与传统批SQL一致，使用无门槛

- 使用两个State来存储已经到达的数据

- 状态持续增长，一般结合stateTTL使用

##### Interval Join

Interval Join是Regular join基础上的优化形式，主要用于实时数据流中需要**联合另一条流中前后一段时间内的数据的情况，要求一条流上有时间区间**。在具体的业务场景中需要明确地知道A表和B表关联数据有先后顺序，且关联的数据有时间范围限制。

在两个流上都需要定义时间属性字段。并在 join 条件中定义左右流的时间区间（**是join on条件上的时间条件限制**）。Interval Join 任意一条流都会触发结果更新，但相比 Regular Join，Interval Join 最大的优点是 state 可以自动清理，根据时间区间保留数据，state 占用大幅减少。Interval Join 适用于业务有明确的时间区间，比如曝光流关联点击流，点击流关联下单流，下单流关联成交流。

Interval Join的特点总结如下：

- 关联中的任意一流都会触发对结果的更新

- State自动清理，根据时间区间保留数据

- 需业务有明确的时间区间，如曝光->点击，点击->下单，下单->成交

##### Temporal join

Temporal join (时态表关联) 是**最常用的数据打宽方式**，它常用来做我们熟知的**维表 Join**。在语法上，它需要一个显式的FOR SYSTEM_TIME AS OF语句。它与Regular Join以及Interval Join最大的区别就是，**维度数据的变化不会触发结果更新**。Flink 支持非常丰富的 Temporal join功能，包括关联lookup DB，关联changelog，关联Hive表。

###### 关联Lookup DB

Temporal Join Lookup DB是**最常见的维表Join方式**，维表的更新不会触发结果的更新，**维度数据存放在数据库中**，适用于实时性要求较高的场景。

> A INNER JOIN FOR SYSTEM_TIME AS OF A.proctime AS B ON A.XX = B.XX，一般使用的是打宽前的表(A)的处理时间，不是维表(B)的处理时间。

> 在一些业务场景下，其他已经打宽的维表组合成的宽表（实施清单基础信息表），在更新频率很低的情况下，也可以作为Temporal Join Lookup DB关联的维表处理。

对于Temporal Join Lookup DB，如果FOR SYSTEM_TIME AS OF关联的时间点为proctime，即处理时间而不是事件时间，**并不一定能够得到维表/数据表的最新结果**。在Flink中，处理时间是指数据被Flink算子处理的系统时间，那么关联的时间点将基于**数据实际到达Flink算子并被处理的时间，这并不意味着将总是获得维表中“最新的”数据，因为“最新”的数据可能还没有到达或者还没有被处理**，主要有这几个原因：

- 时间偏差：由于网络延迟、系统负载等原因，不同事件的处理时间可能会有所偏差。这可能导致Temporal Join的结果不准确。

- 无法处理乱序事件：如果事件是乱序的（即它们没有按照事件时间排序），那么使用处理时间进行Temporal Join可能会导致错误的关联结果。

- 数据版本不一致：在分布式系统中，即使使用处理时间，也不能保证在所有节点上都使用完全相同的维表数据进行Temporal Join，特别是当维表数据正在更新时。

在大部分维表join的场景中，维表的更新基本是频率很低的，新的数据到达=处理完毕，可以理解为没有上述问题存在，但在另外一些场景中，如参考文章写的直播间互动数据和直播间维度数据，是完全有可能因为链路过长导致维表处理时间延迟造成Temporal Join关联不上的问题的。

因此，如果希望关联维表数据的准确可靠性，可以使用**事件时间+Watermark机制**来处理乱序事件和延迟到达的数据，从而确保Temporal Join的准确性。

Temporal Join Lookup DB的特点总结如下：

- 维度表的更新不触发结果的更新

- 维度数据存储在数据库中

- 适用于实时性要求较高的场景（维表基本为最新数据）

- 一般会开启Async和内存提升查询效率

###### 关联Changelog

如Temporal Join Lookup DB里面最后说的，在一些准确性要求较高的场景下，同时创建的维表数据和关联的数据会因为维表数据的延迟（链路过长等）而造成无法匹配上，造成统计误差。

针对这类场景，Flink 1.12支持了Temporal Join Changelog，通过**从changelog在 Flink state 中物化出维表来实现维表关联**。

这里FOR SYSTEM_TIME AS OF不是跟一个processing time（处理时间），而是**左流的 event time（事件时间），它的含义是去关联这个event time时刻的左表数据**，同时在维表的upsert流上也定义了watermark，所以temporal join changelog在执行上会做watermark等待和对齐，保证关联上精确版本的结果，从而解决先前方案中关联不上的问题。

```html
Watermark在Flink中是一种特殊的时间戳，用于表示eventTime小于watermark的事件已经全部落入到相应的窗口中，此时可进行窗口操作。Watermark的作用主要有：

- 事件时间对齐：在分布式系统中，由于网络延迟、系统负载等原因，不同节点上的事件时间可能存在偏差。Watermark可以帮助系统识别出已经到达的最早时间，从而实现事件时间的对齐。

- 触发窗口操作：在基于事件时间的窗口操作中，Watermark的推进可以触发窗口的关闭和计算。当Watermark超过窗口的结束时间时，该窗口内的数据将被视为完整并触发计算。

- 状态清理：对于Interval Join等需要缓存状态的操作，Watermark的推进还可以帮助系统识别并清理过期的状态数据，释放内存资源。

Watermark的生成时机通常在接收到source的数据后立刻生成，但也可以在source后应用简单的map或filter操作后再生成。Watermark的类型包括有序流的watermarks、无序流的watermarks和多并行流的watermarks等。在多并行度的情况下，watermark对齐会取所有channel最小的watermark。
```

Temporal Join Changelog的特点是实时性高，因为是**按照event time做的版本关联，所以能关联上精确版本的信息，且维表会做watermark对齐等待，使得用户可以通过watermark控制“迟到”的维表数据**。Temporal Join Changelog中的维表数据都是存放在temporal join节点的state 中，读取非常高效，就像是一个本地的Redis一样，用户不再需要维护额外的Redis组件。

> 本项目中绝大多数Temporal Join都是关联Lookup DB，没有准确性关联要求特别高的场景，因此关联Changelog主要是作者个人理解。

Temporal Join Changelog的特点总结如下：

- 实时性高，精确版本关联

- 维表设置watermark（等待时间对齐）

- 维度数据存在Temporal Join State中

###### 关联Hive

在数仓场景中，Hive的使用是非常广泛的，Flink与Hive的集成非常友好，可以支持Temporal Join Hive分区表和非分区表。

Temporal Join Hive的特点是**可以自动读取Hive的最新分区数据**，比如订单流关联店铺数据，店铺数据可以认为是**更新缓慢的维表**，业务方每天更新一次，全量写入Hive分区表，通过设置参数，Flink可以监控到新分区并重新加载当天数据到cache中并替换掉昨天的数据作为最新的维表进行关联计算。相比于Temporal Join Lookup DB从外部源读取维表最新数据的方式，减少了IO的开销。

![7](https://i.postimg.cc/BvrPpL6Z/A35-AA146-DB94-40d5-A08-B-17-E438-DC60-C2.png)

Temporal Join Hive的特点总结如下：

- 自动关联Hive维表最新分区

- 适用于维表更新缓慢情景

- 高吞吐

> IO开销和吞吐量成反比，吞吐量越高，单位时间内处理的数据量越大；时效性越好，单个查询响应越快

###### Flink打宽join总结

![8](https://i.postimg.cc/63C2mW2r/37-B4-F768-B146-41d2-BB2-B-843891366-E9-B.png)

- Regular Join的实效性非常高，吞吐一般，因为state会保留所有到达的数据，适用于双流关联场景；
Interval Join的时效性非常好，吞吐较好，因为state只保留时间区间内的数据，适用于有业务时间区间的双流关联场景；

- Temporal Join Lookup DB的时效性比较好，吞吐较差，因为每条数据都需要查询外部系统，会有IO开销，适用于维表在数据库中的场景；

- Temporal Join Changelog的时效性很好，吞吐也比较好，因为它没有IO开销，适用于需要维表等待，或者关联准确版本的场景；

- Temporal Join Hive的时效性一般，但吞吐非常好，因为维表的数据存放在cache中，适用于维表缓慢更新的场景，高吞吐的场景。

#### 一些故障和技术问题及解决方法

内存公式

```html
yarn-session.sh -s 5 -jm 4096 -nm xxx -d -D taskmanager.memory.managed.size=128m -D taskmanager.memory.process.size=30720m -D taskmanager.memory.network.max=128m -D taskmanager.memory.jvm-overhead.max=256m
-tm 4096
SET 'parallelism.default' = '35';

最终内存公式：
'parallelism.default' = P 
-s = Q
N * taskmanager(memory) + 1 * jobmanager(memory) 
= P/Q * max(tm, taskmanager.memory.process.size) + 1 * jobmanager(memory) 
有时P/Q会+1，暂时未知

slot的个数决定了可以支持多少个算子并行，即相同算子针对不同数据在几个slot中运行
每个taskmanager的slot个数决定了taskmanager能够执行的任务数量
```

##### TiDB、TBDS故障类(TiDB网络波动、TiDB CPU使用过高、TiDB服务器重启、TBDS组件异常、TBDS安全漏洞升级重启、HDFS宕机)

yarn-session提交，主从切换->binlog中断，Flink任务重启，latest-offset，人工补历史数据（interval）。TBDS元数据丢失，重建任务。

最终：cdc表锁死物理IP，不再使用vip，减少宕机、主从切换的影响。

##### 离线存储过程和实时任务改造

办件明细->办件统计

离线改造（2-3小时->10分以内，依靠单条查询循环）

##### Flink算子问题（开窗函数计算结果不一致）

5.7版本没有窗口函数，使用Flink SQL的 row_number() over，拓展为流计算问题？

##### 脱离数据中台的数据质量和接入产出监控

各种存储过程更新有问题统计（上游物理删除、业务系统数据异常，保证明细和统计结果一致）

### 总结和展望

### 参考

