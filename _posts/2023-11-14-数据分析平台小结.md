---
layout:     post
title:      数据分析平台小结
subtitle:
date:       2023-11-14
author:     RC
header-img:
catalog: true
tags:
    - 数据仓库
    - FlinkCDC
    - 数据分析
    - 大数据
    - 流计算
---

### 项目背景

本文是针对成都市“蓉易办”数据分析管理系统（以下简称数据分析平台）中技术架构、数据仓库建设和数据分析工作的内容的小结。

数据分析平台的主要数据来源为“天府蓉易办”平台和省一体化平台的政务数据。

“天府蓉易办”平台是成都按照国家和四川省关于一体化在线政务服务平台的相关要求，在吸收借鉴先进城市优秀经验基础上，打造的智能化政务服务一网通办平台，让企业和群众办事 “上一张网，最多跑一次、 一次能办成” 。同时，也是成都市优化营商环境的重要品牌。

> 本文编写时间为2023年11月，蓉易办1期项目已于2023年10月验收完毕

数据分析平台建设目的主要有：

- 解决数据实时同步问题，将业务数据及时同步给数据分析管理平台使用
	 
- 建立蓉易办数据仓库，实现数据来源、数据治理、数据共享统一管理
	 
- 实现办件、排对叫号、好差评、事项、行政区划和部门等业务、指标数据实时同步，实时计算和离线计算
	 
本人根据公司安排参与了数据分析平台自2023年1月至今的数仓建设、数据分析、数据共享、故障处理等工作，因工作中首次涉及到了Flink CDC、TDSQL等组件的使用，遇到了一些问题并采取了解决办法，所以编写了本文做一个阶段性的技术总结。

### 数据分析平台技术架构

数据分析平台整体的技术架构如下：

![1](https://i.postimg.cc/kXxWHx4M/image.jpg)

可以看到比较的轻型且容易理解，没有任何中间件，是一个典型的用Flink+TDSQL实现的流批一体、实时计算的OLAP架构。

总的流程为：

业务系统、省一体化平台等数据源将数据同步到TDSQL形成的前置库， 数据分析平台通过Flink CDC实时捕获数据变化，经过实时计算同步、加工数据到数据仓库各层（ODS, DWD, DIM, ADS层）。实时计算处理实时业务需求，离线计算处理非实时业务需求（如业务统计指标计算），最终提供数据到蓉易办数据管理分析平台用于统计分析、支撑大屏数据展示、支持其他平台的数据共享交换等。

> 因TDSQL本身是一个分布式的类MySQL数据库，因此提供了binlog，这使得Flink CDC不需要任何额外的组件就可以通过binlog捕获所有数据的状态变化和更新，达到流计算的要求。

> 但同时，我们应该清楚的认识到，作为实时计算的代价，基于Flink CDC的流计算任务根据计算的复杂度和表的大小所耗费的内存可能极大

### Flink CDC简介

#### 数据接入
	
数据仓库的数据来源主要是日志和数据库，其中数据库接入比较复杂，相关的工具包括Canal，Debezium，Maxwell。Flink通过CDC format与这些同步工具做了很好的集成，可以**直接消费这些同步工具产生的数据**：

![5](https://i.postimg.cc/VkP32Qm3/4-B1903-F2-C896-4251-BE64-C486-AC7-E82-B7.png)

同时Flink还推出了**原生的CDC connector，直连数据库**，降低接入门槛，简化数据同步流程。

在最初的常见设计中，通过 Debezium或者Canal去实时采集MySQL数据库的 binlog，并将行级的变更事件同步到Kafka中供Flink分析处理。**在Flink推出CDC format之前，用户要去消费这种数据会非常麻烦**，用户需要了解CDC工具的数据格式，将before，after等字段都声明出来，然后用ROW_NUMBER做个去重，来保证实时保留最后一行的语义。但这样使用**成本很高，而且也不支持 DELETE 事件**。

现在Flink支持了CDC format，我们在源CDC表的建表语句中的with参数中可以直接指定format = ‘debezium-json’，然后schema部分只需要填数据库中表的schema即可。Flink能自动识别Debezium 的 INSERT/UPDATE/DELETE事件，并转成Flink内部的INSERT/UPDATE/DELETE消息。**我们可以在该表上直接做聚合、join等操作，就跟操作一个MySQL实时物化视图一样，非常方便**：

![2](https://i.postimg.cc/pTBfVjFf/592e206c4baf41bca2c7086a842780c5.png)

**"MySQL实时物化视图"**

FlinkSQL定义的动态表，动态表和流的概念是对等的。参照下图，流可以转换成动态表，动态表也可以转换成流：

![3](https://i.postimg.cc/HLVQ3bwk/B5-C79-F14-6836-4f04-B9-FA-957-E141-BAD8-D.png)

在Flink中，数据在从一个算子流向另外一个算子时都是以Changelog Stream的形式，任意时刻的Changelog Stream可以翻译为一个表，也可以翻译为一个流。联想下MySQL中的表和binlog日志，就会发现：MySQL数据库的一张表所有的变更都记录在binlog日志中，如果一直对表进行更新，binlog日志流也一直会追加，数据库中的表就相当于binlog日志流在某个时刻点物化的结果；日志流就是将表的变更数据持续捕获的结果。这说明Flink的Dynamic Table是可以**非常自然地表示一张不断变化的MySQL数据库表**。

> 项目上的Flink版本为1.13.2，mysql-cdc版本为2.2.0

#### Flink CDC 同步原理

新版本中，Flink内部原生支持了CDC的语义，所以可以很自然地直接去读取MySQL的binlog数据并转成Flink内部的变更消息。

利用MySQL CDC connector，你只需要在with参数中指定**connector=mysql-cdc**，然后select这张表就能实时读取MySQL中的全量+CDC增量数据，无需部署其他组件和服务。Flink中定义的这张表可以理解成是MySQL的实时物化视图（如上文所述），所以在这张表上的聚合、join等结果，跟实时在MySQL中运行出来的结果是一致的。

相比于之前设计中的Debezium，Canal的架构，CDC connector在使用上更加简单易用了，不用再去学习和维护额外组件，数据不需要经过Kafka落地，减少了端到端延迟。而且**支持先读取全量数据，并无缝切换到CDC增量读取上，也就是我们说的是流批一体，流批融合的架构**。

MySQL CDC connector非常受用户的欢迎，尤其是**结合OLAP引擎（Doris、ClickHouse、TiDB等），可以快速构建实时OLAP架构**。实时OLAP架构的一个特点就是将数据库数据同步到OLAP中做即席查询，这样就**无需离线数仓**了。

**为什么是流批一体？**

> 先看看之前设计中的流计算方式：用 datax 做个全量同步，然后用 canal 同步实时增量到 Kafka，然后从 Kafka 同步到 OLAP，这种架构比较复杂，链路也很长。

以Flink+ClickHouse为例，**只需要在 Flink 中定义一个MySQL-CDC source，一个ClickHouse sink，然后提交一个insert into query的Flink SQL，就完成了从MySQL到ClickHouse的实时同步工作**，非常方便。而且，ClickHouse有一个痛点就是join比较慢，所以一般我们会把MySQ 数据打成一张大的明细宽表数据，再写入 ClickHouse。这个**打宽的动作在 Flink 中一个join操作就完成了**：

![6](https://i.postimg.cc/DwzY92Rp/B70-D876-B-0931-4e72-9-BE9-CF5-E155-B19-E3.png)

> 在Flink提供MySQL CDC connector之前，要在全量+增量的实时同步过程中做 join 是非常麻烦的

> 当然，根据join的表大小和复杂程度，需要的join内存也是很大的

本项目中，只是将source和sink换成了TDSQL source、TDSQL Sink(TDSQL有完整的Binlog)，加上insert into query的Flink SQL，就完成了Flink+TDSQL快速构建实时OLAP架构的数据仓库。

> 这里的source表'connector'='mysql-cdc'，sink表'connector'='jdbc'（因为写入的OLAP是TDSQL）

> 对于一些无法支持原生CDC的数据源，只能通过'connector'='kafka'，'format'='debezium-json'等接入，但是sink的connector支持很多类型，比如绝大多数的OLAP数据库（Doris、ClickHouse、TiDB、TDSQL等），以及Hive、HBase、Iceberg、upsert-kafka

![4](https://i.postimg.cc/htwJD6mK/40-C866-FF-E05-E-4622-A83-E-C980-CDD27346.png)

该例子是通过Flink CDC去同步数据库数据并写入到TiDB，用户直接使用Flink SQL创建了产品和订单的MySQL-CDC表，然后对数据流进行join加工，加工后直接写入到下游数据库。**通过一个Flink SQL作业就完成了CDC的数据分析，加工和同步**。

并且这个同步和加工动作是一个**纯SQL作业**，这意味着只要会SQL的BI、业务线同学都可以完成此类工作。与此同时，用户也可以利用Flink SQL提供的丰富语法进行数据清洗、分析、聚合。

总结，利用Flink CDC快速构建实时OLAP架构的优势有：

- **简单易用，纯SQL作业**

- **没有额外中间件，数据不落地，减少了存储成本和数据延迟**

- **支持全量读取+CDC增量读取，完全的无缝切换，流批一体**

- **支持Exactly Once读取和处理，数据一致**

- **明细宽表直接入OLAP，统一流批存储，无需离线数据仓**

- **链路更短更直接，应用层接入成本降低，灵活应对需求变更**


#### 常见的打宽操作

##### Regular Join

Regular Join即为双流join，Regular Join的特点是，**任意一侧流的数据更新都会触发结果的更新**（都会重新计算一次），其语法与传统的批处理SQL一致。

Regular join涉及到两个或多个流的联接，因此需要**在内存中存储与这些流相关的数据以便进行联接操作，通过state来存储双流已经到达的数据**，state默认永久保留（实际上记录了两个join流的数据的全部状态）。因此为了管理这些状态并防止内存溢出，Flink引入了TTL机制。

TTL定义了与Regular join操作相关联的状态在内存中可以被保留的最长时间。一旦状态数据超过了这个时间限制，Flink就会自动清理这些数据，释放内存空间。**TTL的值可以根据具体的业务需求和系统的性能要求进行设置**。较大的TTL值意味着状态数据可以在内存中保留更长的时间，从而可能提高查询的准确性和效率，但也可能增加内存使用的压力。相反，较小的TTL值可以减少内存使用，但可能降低查询的准确性或增加延迟。

> 如果两个流之间的数据到达速率严重不平衡，或者存在大量的重复数据，那么即使设置了TTL也可能无法完全避免内存溢出的问题。

> 在Flink中，TTL可以应用于不同类型的state，包括ValueState、ListState、MapState等。对于Regular join操作，通常需要根据具体的实现和需求来确定应该为哪种类型的state设置TTL。

##### Interval Join

Interval Join是Regular join基础上的优化形式，主要用于实时数据流中需要**联合另一条流中前后一段时间内的数据的情况，要求一条流上有时间区间**。在具体的业务场景中需要明确地知道A表和B表关联数据有先后顺序，且关联的数据有时间范围限制。

在两个流上都需要定义时间属性字段。并在 join 条件中定义左右流的时间区间（**是join on条件上的时间条件限制**）。Interval Join 任意一条流都会触发结果更新，但相比 Regular Join，Interval Join 最大的优点是 state 可以自动清理，根据时间区间保留数据，state 占用大幅减少。Interval Join 适用于业务有明确的时间区间，比如曝光流关联点击流，点击流关联下单流，下单流关联成交流。

##### Temporal join

Temporal join (时态表关联) 是**最常用的数据打宽方式**，它常用来做我们熟知的**维表 Join**。在语法上，它需要一个显式的FOR SYSTEM_TIME AS OF语句。它与Regular Join以及Interval Join最大的区别就是，**维度数据的变化不会触发结果更新**。Flink 支持非常丰富的 Temporal join功能，包括关联lookup DB，关联changelog，关联Hive表。

###### 关联Lookup DB

Temporal Join Lookup DB是**最常见的维表Join方式**，维表的更新不会触发结果的更新，**维度数据存放在数据库中**，适用于实时性要求较高的场景。

> A INNER JOIN FOR SYSTEM_TIME AS OF A.proctime AS B ON A.XX = B.XX，一般使用的是打宽前的表(A)的处理时间，不是维表(B)的处理时间。

> 在一些业务场景下，其他已经打宽的维表组合成的宽表（实施清单基础信息表），在更新频率很低的情况下，也可以作为Temporal Join Lookup DB关联的维表处理。

对于Temporal Join Lookup DB，如果FOR SYSTEM_TIME AS OF关联的时间点为proctime，即处理时间而不是事件时间，**并不一定能够得到维表/数据表的最新结果**。在Flink中，处理时间是指数据被Flink算子处理的系统时间，那么关联的时间点将基于**数据实际到达Flink算子并被处理的时间，这并不意味着将总是获得维表中“最新的”数据，因为“最新”的数据可能还没有到达或者还没有被处理**，主要有这几个原因：

- 时间偏差：由于网络延迟、系统负载等原因，不同事件的处理时间可能会有所偏差。这可能导致Temporal Join的结果不准确。

- 无法处理乱序事件：如果事件是乱序的（即它们没有按照事件时间排序），那么使用处理时间进行Temporal Join可能会导致错误的关联结果。

- 数据版本不一致：在分布式系统中，即使使用处理时间，也不能保证在所有节点上都使用完全相同的维表数据进行Temporal Join，特别是当维表数据正在更新时。

在大部分维表join的场景中，维表的更新基本是频率很低的，新的数据到达=处理完毕，可以理解为没有上述问题存在，但在另外一些场景中，如参考文章写的直播间互动数据和直播间维度数据，是完全有可能因为链路过长导致维表处理时间延迟造成Temporal Join关联不上的问题的。

因此，如果希望关联维表数据的准确可靠性，可以使用**事件时间+Watermark机制**来处理乱序事件和延迟到达的数据，从而确保Temporal Join的准确性。

###### 关联Changelog




#### 一些故障和解决方法

### 总结和展望

### 参考

