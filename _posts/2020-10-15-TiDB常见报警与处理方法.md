---
layout:     post
title:      TiDB常见报警与处理方法
subtitle:  	
date:       2020-10-15
author:     RC
header-img: 
catalog: true
tags:
    - TiDB
    - Prometheus
    - 故障处理
---

## 正文
TiDB从测试环境到上线已经有一段时间了，遇到的报警不少，将其中常见的监控告警和基本处理方法总结如下

```html
严重程度:警告
告警名称:TiDB tikvclient_backoff_count error 
指标:increase( tidb_tikvclient_backoff_seconds_count[10m] )
值:> 1000
1728%指当前的10分钟内错误次数为1728次
说明:
指10分钟内发生write conflict时,tidb访问tikv的重试次数
通常是由于 region 或 leader 调度引起
措施:
观察业务是否在短期内有较高的并发写入或者更新操作
tiboard->SQL语句分析->最近30分钟->选中insert/update
extra:
无
```

```html
严重程度:警告
告警名称:TiDB heap memory usage is over 10 GB
指标:go_memstats_heap_inuse_bytes{job="tidb"}
值:"> 1e+10
1.203%指tidb当前使用的内存超过了10.12G"
说明:
"指tidb当前使用的内存大小
通常是由于大事务引起"
措施:
"在tiboard上的日志搜索中查找关键字expensive
找到具体使用情况和SQL语句"
extra:
无
```

```html
严重程度:紧急
告警名称:TiDB monitor_keep_alive error
指标:increase(tidb_monitor_keep_alive_total{job="tidb"}[10m])
值:" 小于100
70.76%指tidb进程在10分钟内的握手成功次数为79.76次"
说明:
"指tidb进程在10分钟内的心跳检测成功次数
通常为tidb异常重启引起"
措施:
"检查tidb是否发生重启（日志grep 'Welcome'）
tidb是否被OOM，服务器是否被重启,网络通信是否有问题"
extra:
无
```

```html
严重程度:警告
告警名称:TiDB query duration 99th percentile is above 1s
指标:histogram_quantile(0.99, sum(rate(tidb_server_handle_query_duration_seconds_bucket[1m])) BY (le, instance))
值:"> 1
15.88%指tidb99%的请求处理超过1.588秒的时间超过1分钟"
说明:
"指1分钟内tidb处理99%请求的延迟
通常为慢查询引起"
措施:
"在tiboard上的日志搜索中查找关键字expensive
找到具体使用情况和SQL语句"
extra:
无
```

```html
严重程度:严重
告警名称:TiDB server panic total 
指标:increase(tidb_server_panic_total[10m])
值:">0
1.025%指10分钟tidb线程崩溃数为1.025"
说明:
"指10分钟内发生崩溃的tidb线程数量
通常会恢复，否则tidb会自动重启"
措施:
"查看tidb日志中panic关键字日志，如""execute sql panic""
至少可以看到崩溃线程执行的SQL"
extra:
无
```

```html
严重程度:严重
告警名称:TiKV server_report_failure_msg_total error
指标:sum(rate(tikv_server_report_failure_msg_total{type="unreachable"}[10m])) BY (store_id)
值:">10
13.58%指10分钟内无法连接tikv的次数为13.58次"
说明:
"指10分钟内无法连接tikv的次数超过10次
如果tikv没有down，则说明tikv压力很大
如果tikv已经重启(grep Welcome)，
一般是因为OOM引起"
措施:
"检查grafana上region是否出现热点问题
检查系统是否出现OOM问题
检查短期内占用内存的SQL"
extra:
"tidb/tidb-TiKV-details/raft read/write proposals per server（分布不均）
Tidb/tidb-TiKV-details/RocksDB-kv/Block cache hit(抖动较大)
Tidb/tidb-TiKV-Trouble-Shooting/hot-read(分布不均)
Tidb/tidb-TiKV-Trouble-Shooting/hot-write（分布不均）
dmesg -T | grep 'Out of memory'
select * from information_schema.tidb_hot_regions where type = 'write'
kill tidb thread_id"
```

```html
严重程度:严重
告警名称:TiKV scheduler latch wait duration seconds more than 1s
指标:histogram_quantile(0.99, sum(rate(tikv_scheduler_latch_wait_duration_seconds_bucket[1m])) by (le, instance, type))
值:">1
14.89%指写操作获取内存锁时的等待时间1分钟内超过14.89秒"
说明:
"指Scheduler 中写操作获取内存锁时的等待时间
直接反映了短期内的写并发度很高/慢查询阻塞
通常是因为热点调度问题并发引起"
措施:
"检查grafana上scheduler的 scheduler scan details指标
如果total 和 process相差较大，则说明有很多无效扫描（索引问题）
如果over seek bound较大，说明内存回收不及时
检查grafana上storage async snapshot/write duration指标，
如果该值较大，说明PD raft操作不及时"
extra:
"TiDB Scheduler 是 Kubernetes 调度器扩展 的 TiDB 实现
tidb/tidb-TiKV-details/scheduler/ scheduler scan details
tidb/tidb-TiKV-details/storage/storage async snapshot/write duration"
```

```html
严重程度:严重
告警名称:TiKV async request write duration seconds more than 1s
指标:histogram_quantile(0.99, sum(rate(tikv_storage_engine_async_request_duration_seconds_bucket{type="write"}[1m])) by (le, instance, type))
值:">1
1.31%指raft write操作在1分钟内超过1.31秒"
说明:
"指raft write操作所消耗的时间
反映了Tikv压力很大，raftstore线程已经卡死
通常是因为热点调度问题并发引起"
措施:
"检查grafana上region是否出现热点问题
检查系统是否出现OOM问题
检查短期内占用内存的SQL"
extra:
同TiKV scheduler latch wait duration seconds more than 1s
```

```html
严重程度:警告
告警名称:TiKV coprocessor request error on not_leader
指标:increase(tikv_coprocessor_request_error{reason!="lock"}[10m])
值:">100
100.5%指tikv在10分钟内Coprocessor 的请求错误次数为100.5"
说明:
"指10分钟内Coprocessor 的请求错误
不为lock的一般为“outdated”和“full”，分别为超时和请求队列已满
基本均由慢查询阻塞引起"
措施:
"在tiboard上的日志搜索中查找关键字expensive
找到具体使用情况和SQL语句
多为leader的迁移引起，即大量写入操作"
extra:
"tikv Coprocessor是TiKV 读取数据并计算的模块，
其功能可以简单类比为MySQL的索引下推，实际上比较复杂
tidb/tidb-TiKV-Summary->Raftstore error->not leader
tidb/tidb-TiKV-details->Raft IO && Raft Propose"
```

```html
严重程度:紧急
告警名称:TiDB schema error
指标:increase(tidb_session_schema_lease_error_total{type="outdated"}[15m])
值:">5
9.152%指tidb在15分钟内在一个单位时间内没能更新tikv信息的次数大于5次"
说明:
"指15分钟内tidb在一个lease的时间内没有重载到最新的 Schema 信息的次数
通常是因为tikv region不可用/超时引起"
措施:
"在grafana监控中查看KV errors是否飙升
tidb duration是否上涨,OPS是否下降
region是否数量上升缓慢
过滤tikv日志找到写冲突的表和SQL"
extra:
"tidb/tidb-tidb/kv errors  ->是否存在serverisbusy->tidb-TiKV-Details/errors定位
tidb/tidb-tidb/Duration-QPS
tidb/tidb-pd/number of regions
cat 218.log| grep conflict | awk -F 'tableID=' '{print $2}' 
select * from information_schema.tables where tidb_table_id='tableID';"
```

```html
严重程度:警告
告警名称:disk_write_latency_more_than_16ms
指标:( (rate(node_disk_write_time_seconds_total{device=~".+"}[5m]) / rate(node_disk_writes_completed_total{device=~".+"}[5m])) or (irate(node_disk_write_time_seconds_total{device=~".+"}[5m]) / irate(node_disk_writes_completed_total{device=~".+"}[5m]))  ) * 1000 
值:">16
103.8%指5分钟内磁盘写延迟平均时间超过32ms"
说明:
"指磁盘写入延迟
关注磁盘利用率（disk utilization）"
措施:
需要通过grafana监控检查各个节点上的磁盘使用情况
extra:
"tidb/tidb-cluster-node_exporter/Disk
Disk IO Util：磁盘使用饱和度"
```

```html
严重程度:警告
告警名称:PD_tidb_handle_requests_duration
指标:histogram_quantile(0.99, sum(rate(pd_client_request_handle_requests_duration_seconds_bucket{type="tso"}[1m])) by (instance,job,le) ) 
值:"> 0.1
0.226%指1分钟内1%pd响应tso请求的时间超过0.1226秒"
说明:
"指PD发起RPC请求的耗时
通常是因为PD负载过高引起"
措施:
"需要检查 TiDB 和 PD 之间的网络延迟是否很高
需要检查PD是否负载太高，不能及时处理 TSO 的 RPC 请求
必要时进行PD leader的手动切换"
extra:
histogram_quantile(分位数),0.99=P99,即1%的响应超过0.1秒,99%的响应少于0.1秒
```

```html
严重程度:警告
告警名称:TiDB ddl waiting_jobs too much
指标:sum(tidb_ddl_waiting_jobs)
值:"> 5
9%指正在等待的DDL操作语句数量为5.45个"
说明:
"指tidb等待执行的DDL语句的个数
通常为并发的truncate语句引起"
措施:
通过admin show ddl jobs查看正在执行的DDL语句
extra:
无
```

```html
严重程度:紧急
告警名称:TiKV memory used too fast
指标:process_resident_memory_bytes{job=~"tikv",instance=~".*"} - (process_resident_memory_bytes{job=~"tikv",instance=~".*"} offset 5m)
值:"> 5*1024*1024*1024
7.099%指5分钟内tikv使用的内存超过了5.35G"
说明:
"指tikv所在服务器5分钟内使用的内存超过5G
通常为rocksdb缓存占用"
措施:
"检查短期内占用内存的SQL
是否有并发的大量查询"
extra:
Tidb/tidb-TiKV-details/RocksDB-kv/Block cache hit(抖动较大)
```

```html
严重程度:警告
告警名称:TiDB monitor time_jump_back error
指标:increase(tidb_monitor_time_jump_back_total[10m])
值:"> 0
48.95%指10分钟内tidb服务器系统时间延迟超过1.49秒"
说明:
"指tidb monitor检查时间是否发生了jump back"
措施:
"如果频繁出现，应检查对应TiDB的系统时间同步是否正常"
extra:
无
```

```html
严重程度:警告
告警名称:PD_cluster_lost_connect_tikv_nums
指标:(sum ( pd_cluster_status{type="store_disconnected_count"} ) by (instance) > 0) and (sum(etcd_server_is_leader) by (instance)
值:" > 0
1指20秒内Pd没有收到tikv的上报心跳"
说明:
"指PD 在 20 秒之内未收到 TiKV 上报心跳。正常情况下是每 10 秒收到 1 次心跳
通常是因为Tikv无法联系，过于繁忙引起"
措施:
"检查TiKV是否重启
TiKV进程是否正常
检查系统是否出现OOM问题
检查短期内占用内存的SQL"
extra:
grep -ir /tidb-deploy/tikv-20160/log/ 'welcome'
ps aux | grep tikv-server
dmesg -T | grep 'Out of memory'
```

```html
严重程度:严重
告警名称:TiKV async request snapshot duration
指标:histogram_quantile(0.99, sum(rate(tikv_storage_engine_async_request_duration_seconds_bucket{type="snapshot"}[1m])) by (le, instance, type))
值:" > 1
5.018%表示1分钟内1%的storage_engine_async_request的时间超过了1.05秒"
说明:
"指 Raftstore 负载压力很大，可能已经卡住"
措施:
"参考TiKV async request write duration seconds more than 1s"
extra:
无
```

```html
严重程度:警告
告警名称:TiKV approximate region size is more than 1GB
指标:histogram_quantile(0.99, sum(rate(tikv_raftstore_region_size_bucket[1m])) by (le))
值:" > 1073741824
4.058%表示TiKV split checker 扫描到的1%的最大的 Region approximate size在1分钟内持续大于1GB"
说明:
"指当前数据写入量较大，region的分裂速度比不上写入的速度"
措施:
"检查当前的写操作SQL
观察业务是否在短期内有较高的并发写入或者更新操作
tiboard->SQL语句分析->最近30分钟->选中insert/update"
extra:
如果还不能缓解，可以手动进行region的分裂
 http://XX.XX.XX.XX:10080/regions/hot --定位hotregion及相关表/索引
SPLIT TABLE table_name [INDEX index_name] BETWEEN (lower_value) AND (upper_value) REGIONS region_num --手动分区
```

```html
严重程度:严重
告警名称:DM sync process exists with error 
指标:changes(dm_syncer_exit_with_error_count[5m])
值:" > 1
1%指5分钟内，dm syncer的报错次数超过1次"
说明:
"指syncer 模块 在 dm-worker 内部遇到错误并且退出了
通常由于DM连接不到tidb或者连接tidb超时引起"
措施:
"该报警通常是因为tidb和tikv繁忙并发引起的
需要密切关注当前的慢查询和tidb压力状况"
extra:
该错误并不会导致DM的同步暂停，重试后等待tidb响应会继续
```

```html
严重程度:严重
告警名称:TiKV_raft_apply_log_duration_secs
指标:histogram_quantile(0.99, sum(rate(tikv_raftstore_apply_log_duration_seconds_bucket[1m])) by (le, instance)) 
值:" > 1
3.549%指1分钟内，1%的apply raft log的耗时超过了1.35秒"
说明:
"指raft应用leader日志花费的时间较长
通常因为IO压力较大引起"
措施:
"检查监控中apply log duration是否较高
确认是哪台tikv的apply延迟较高"
extra:
tidb/tidb-tikv-details/Raft IO：apply log duration
apply log duration per server
观察当前热点更新类慢查询
```

```html
严重程度:紧急
告警名称:TiKV_GC_can_not_work 
指标:sum(increase(tidb_tikvclient_gc_action_result{type="success"}[6h])) 
值:"< 1
指tidb在6小时内region上没有成功执行1次GC"
说明:
"指GC不能正常工作了
现象为tidb_tikvclient_gc_action_result持续减少,最后降为0
目前出现该情况的原因未知"
措施:
"首先确认gc leader 对应的 tidb-server
然后通过该台服务器的日志,查看GC进程是否正常工作
若已经失效,需要重启tiKV角色"
extra:
select VARIABLE_VALUE from mysql.tidb where VARIABLE_NAME="tikv_gc_leader_desc";
查看该 tidb-server 的日志,grep gc_worker tidb.log
观察日志中相关内容是否最近在resolve locks或者delete ranges
若确实失效,通过tiup cluster reload tidb -R tikv重启tidb角色
```

可以看到TiDB维护起来坑还是不少的，需要持续学习和总结。

