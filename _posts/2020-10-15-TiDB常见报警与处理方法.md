---
layout:     post
title:      TiDB常见报警与处理方法
subtitle:  	
date:       2020-10-15
author:     RC
header-img: 
catalog: true
tags:
    - TiDB
    - Prometheus
    - 故障处理
---

## 正文
TiDB从测试环境到上线已经有一段时间了，遇到的报警不少，将其中常见的监控告警和基本处理方法总结如下

```html
严重程度:警告
告警名称:TiDB tikvclient_backoff_count error 
指标:increase( tidb_tikvclient_backoff_seconds_count[10m] )
值:> 1000
1728%指当前的10分钟内错误次数为1728次
说明:
指10分钟内发生write conflict时,tidb访问tikv的重试次数
通常是由于 region 或 leader 调度引起
措施:
观察业务是否在短期内有较高的并发写入或者更新操作
tiboard->SQL语句分析->最近30分钟->选中insert/update
extra:
无
```

```html
严重程度:警告
告警名称:TiDB tikvclient_backoff_count error 
指标:increase( tidb_tikvclient_backoff_seconds_count[10m] )
值:> 1000
1728%指当前的10分钟内错误次数为1728次
说明:
指10分钟内发生write conflict时,tidb访问tikv的重试次数
通常是由于 region 或 leader 调度引起
措施:
"在tiboard上的日志搜索中查找关键字expensive
找到具体使用情况和SQL语句"
extra:
无
```

```html
严重程度:紧急
告警名称:TiDB monitor_keep_alive error
指标:increase(tidb_monitor_keep_alive_total{job="tidb"}[10m])
值:" < 100
70.76%指tidb进程在10分钟内的握手成功次数为79.76次"
说明:
"指tidb进程在10分钟内的心跳检测成功次数
通常为tidb异常重启引起"
措施:
"检查tidb是否发生重启（日志grep 'Welcome'）
tidb是否被OOM，服务器是否被重启,网络通信是否有问题"
extra:
无
```

```html
严重程度:警告
告警名称:TiDB query duration 99th percentile is above 1s
指标:histogram_quantile(0.99, sum(rate(tidb_server_handle_query_duration_seconds_bucket[1m])) BY (le, instance))
值:"> 1
15.88%指tidb99%的请求处理超过1.588秒的时间超过1分钟"
说明:
"指1分钟内tidb处理99%请求的延迟
通常为慢查询引起"
措施:
"在tiboard上的日志搜索中查找关键字expensive
找到具体使用情况和SQL语句"
extra:
无
```

```html
严重程度:严重
告警名称:TiDB server panic total 
指标:increase(tidb_server_panic_total[10m])
值:">0
1.025%指10分钟tidb线程崩溃数为1.025"
说明:
"指10分钟内发生崩溃的tidb线程数量
通常会恢复，否则tidb会自动重启"
措施:
"查看tidb日志中panic关键字日志，如""execute sql panic""
至少可以看到崩溃线程执行的SQL"
extra:
无
```

```html
严重程度:严重
告警名称:TiKV server_report_failure_msg_total error
指标:sum(rate(tikv_server_report_failure_msg_total{type="unreachable"}[10m])) BY (store_id)
值:">10
13.58%指10分钟内无法连接tikv的次数为13.58次"
说明:
"指10分钟内无法连接tikv的次数超过10次
如果tikv没有down，则说明tikv压力很大
如果tikv已经重启(grep Welcome)，
一般是因为OOM引起"
措施:
"检查grafana上region是否出现热点问题
检查系统是否出现OOM问题
检查短期内占用内存的SQL"
extra:
"tidb/tidb-TiKV-details/raft read/write proposals per server（分布不均）
Tidb/tidb-TiKV-details/RocksDB-kv/Block cache hit(抖动较大)
Tidb/tidb-TiKV-Trouble-Shooting/hot-read(分布不均)
Tidb/tidb-TiKV-Trouble-Shooting/hot-write（分布不均）
dmesg -T | grep 'Out of memory'
select * from information_schema.tidb_hot_regions where type = 'write'
kill tidb thread_id"
```

```html
严重程度:严重
告警名称:TiKV scheduler latch wait duration seconds more than 1s
指标:histogram_quantile(0.99, sum(rate(tikv_scheduler_latch_wait_duration_seconds_bucket[1m])) by (le, instance, type))
值:">1
14.89%指写操作获取内存锁时的等待时间1分钟内超过14.89秒"
说明:
"指Scheduler 中写操作获取内存锁时的等待时间
直接反映了短期内的写并发度很高/慢查询阻塞
通常是因为热点调度问题并发引起"
措施:
"检查grafana上scheduler的 scheduler scan details指标
如果total 和 process相差较大，则说明有很多无效扫描（索引问题）
如果over seek bound较大，说明内存回收不及时
检查grafana上storage async snapshot/write duration指标，
如果该值较大，说明PD raft操作不及时"
extra:
"TiDB Scheduler 是 Kubernetes 调度器扩展 的 TiDB 实现
tidb/tidb-TiKV-details/scheduler/ scheduler scan details
tidb/tidb-TiKV-details/storage/storage async snapshot/write duration"
```

```html
严重程度:严重
告警名称:TiKV async request write duration seconds more than 1s
指标:histogram_quantile(0.99, sum(rate(tikv_storage_engine_async_request_duration_seconds_bucket{type="write"}[1m])) by (le, instance, type))
值:">1
1.31%指raft write操作在1分钟内超过1.31秒"
说明:
"指raft write操作所消耗的时间
反映了Tikv压力很大，raftstore线程已经卡死
通常是因为热点调度问题并发引起"
措施:
"检查grafana上region是否出现热点问题
检查系统是否出现OOM问题
检查短期内占用内存的SQL"
extra:
同TiKV scheduler latch wait duration seconds more than 1s
```

```html
严重程度:警告
告警名称:TiKV coprocessor request error on not_leader
指标:increase(tikv_coprocessor_request_error{reason!="lock"}[10m])
值:">100
100.5%指tikv在10分钟内Coprocessor 的请求错误次数为100.5"
说明:
"指10分钟内Coprocessor 的请求错误
不为lock的一般为“outdated”和“full”，分别为超时和请求队列已满
基本均由慢查询阻塞引起"
措施:
"在tiboard上的日志搜索中查找关键字expensive
找到具体使用情况和SQL语句"
extra:
"tikv Coprocessor是TiKV 读取数据并计算的模块，
其功能可以简单类比为MySQL的索引下推，实际上比较复杂"
```

```html
严重程度:紧急
告警名称:TiDB schema error
指标:increase(tidb_session_schema_lease_error_total{type="outdated"}[15m])
值:">5
9.152%指tidb在15分钟内在一个单位时间内没能更新tikv信息的次数大于5次"
说明:
"指15分钟内tidb在一个lease的时间内没有重载到最新的 Schema 信息的次数
通常是因为tikv region不可用/超时引起"
措施:
"在grafana监控中查看KV errors是否飙升
tidb duration是否上涨,OPS是否下降
region是否数量上升缓慢
过滤tikv日志找到写冲突的表和SQL"
extra:
"tidb/tidb-tidb/kv errors  ->是否存在serverisbusy->tidb-TiKV-Details/errors定位
tidb/tidb-tidb/Duration-QPS
tidb/tidb-pd/number of regions
cat 218.log| grep conflict | awk -F 'tableID=' '{print $2}' 
select * from information_schema.tables where tidb_table_id='tableID';"
```

```html
严重程度:警告
告警名称:disk_write_latency_more_than_16ms
指标:( (rate(node_disk_write_time_seconds_total{device=~".+"}[5m]) / rate(node_disk_writes_completed_total{device=~".+"}[5m])) or (irate(node_disk_write_time_seconds_total{device=~".+"}[5m]) / irate(node_disk_writes_completed_total{device=~".+"}[5m]))  ) * 1000 
值:">16
103.8%指5分钟内磁盘写延迟平均时间超过32ms"
说明:
"指磁盘写入延迟
关注磁盘利用率（disk utilization）"
措施:
需要通过grafana监控检查各个节点上的磁盘使用情况
extra:
"tidb/tidb-cluster-node_exporter/Disk
Disk IO Util：磁盘使用饱和度"
```

```html
严重程度:警告
告警名称:PD_tidb_handle_requests_duration
指标:histogram_quantile(0.99, sum(rate(pd_client_request_handle_requests_duration_seconds_bucket{type="tso"}[1m])) by (instance,job,le) ) 
值:"> 0.1
0.226%指1分钟内1%pd响应tso请求的时间超过0.1226秒"
说明:
"指PD发起RPC请求的耗时
通常是因为PD负载过高引起"
措施:
"需要检查 TiDB 和 PD 之间的网络延迟是否很高
需要检查PD是否负载太高，不能及时处理 TSO 的 RPC 请求
必要时进行PD leader的手动切换"
extra:
histogram_quantile(分位数),0.99=P99,即1%的响应超过0.1秒,99%的响应少于0.1秒
```

```html
严重程度:警告
告警名称:TiDB ddl waiting_jobs too much
指标:sum(tidb_ddl_waiting_jobs)
值:"> 5
9%指正在等待的DDL操作语句数量为5.45个"
说明:
"指tidb等待执行的DDL语句的个数
通常为并发的truncate语句引起"
措施:
通过admin show ddl jobs查看正在执行的DDL语句
extra:
无
```

```html
严重程度:紧急
告警名称:TiKV memory used too fast
指标:process_resident_memory_bytes{job=~"tikv",instance=~".*"} - (process_resident_memory_bytes{job=~"tikv",instance=~".*"} offset 5m)
值:"> 5*1024*1024*1024
7.099%指5分钟内tikv使用的内存超过了5.35G"
说明:
"指tikv所在服务器5分钟内使用的内存超过5G
通常为rocksdb缓存占用"
措施:
"检查短期内占用内存的SQL
是否有并发的大量查询"
extra:
Tidb/tidb-TiKV-details/RocksDB-kv/Block cache hit(抖动较大)
```

可以看到TiDB维护起来坑还是不少的，需要持续学习和总结。

