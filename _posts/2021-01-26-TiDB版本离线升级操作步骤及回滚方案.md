---
layout:     post
title:      TiDB版本离线升级操作步骤及回滚方案
subtitle:  	
date:       2021-01-26
author:     RC
header-img: 
catalog: true
tags:
    - TiDB
    - 升级
    - 回滚方案
---

### 1. 升级背景
随着4.0.10版本的发布，5.0GA也在1月23号上线了。

考虑到今后的持续使用，在测试环境测试完毕后，对线上TiDB集群进行升级，版本从4.0.0->4.0.9

升级过程本身并不困难，主要是如何保证升级失败后回滚时数据完整，即完善的回滚方案。

### 2. 准备工作

因为TiDB相关节点均在广州机房，为内网环境，因此需要进行离线升级

#### 1.1 安装包
下载新版本离线镜像包tidb-community-server-v4.0.9-linux-amd64.tar.gz并上传到广州中控机节点6.69

MD5校验值:2058dcd5b4ba982c2581614bc44bbd30

#### 1.2 确认SSH互信
确认中控机节点6.69和其他9个节点的root用户及tidb用户的免SSH登陆互信

#### 1.3 确认tiup工具版本
确认中控机节点6.69的tiup工具版本，如不为1.3.1，则需要升级

### 3. 升级步骤

#### 3.1 配置v4.0.9镜像

解压镜像包并切换当前镜像

```html
$ tar -xvzf tidb-community-server-v4.0.9-linux-amd64.tar.gz
$ cd tidb-community-server-v4.0.9-linux-amd64
$ sh local_install.sh
```
正确应返回

```html
Set mirror to /Install/tidb-community-server-v4.0.9-linux-amd64 success
```

#### 3.2 检查tiup版本

```html
$ tiup --version
--若不为1.3.1，则进行升级
$ tiup update cluster
$ tiup --version
$ tiup cluster display tidb
```

> 实际上，切换镜像后TiUP版本自动变为高版本

#### 3.3 关闭群集，进行离线升级并滚动重启

```html
$ tiup cluster stop tidb
$ tiup cluster display tidb
$ tiup cluster upgrade tidb v4.0.9 --force
--等待升级完成，正常重启顺序为pd->tikv->tidb
$ tiup cluster display tidb
```

### 4. 验证升级是否成功

```html
--4.0.8新特性估计函数
use ehai;
select APPROX_PERCENTILE(createtime, 50) from ECAddedServiceLog;
--4.0.9 tidb dashborad新特性拓扑图缩放
```

### 5. 升级中可能发生的错误和处理方法

#### 5.1 dial tcp connection refused错误
```html
Error: failed to get PD leader 172.xx.xx.172: no endpoint available, the last err was: Get http://172.17.30.174:2379/pd/api/v1/leader: dial tcp 172.17.30.174:2379: connect: connection refused
加入--force后理论上不会出现，需要检查tidb和root账户是否已经互信
```

#### 5.2 压缩文件损坏

```html
Error: stderr: gzip: stdin: invalid compressed data--crc error
中控机SCP新版本软件文件包解压时发现文件损坏
需要检查win下载的包和中控机的包的MD5值是否一致，如果不一致，说明目标节点的磁盘可能出现故障
测试环境中已经升级成功的4.0.9的包的MD5值：
2058dcd5b4ba982c2581614bc44bbd30
Windows中的MD5值：
2058dcd5b4ba982c2581614bc44bbd30
```
#### 5.3 KV节点报错region地址已被占用

```
某节点组件损坏(kv region地址被占用，重复)
--下线该节点
tiup cluster scale-in tidb --node 172.xx.xx.172:2379 –force
之后重新加入(单次加入一个节点)：
vim scale-out.yaml
tikv_servers:
- host: 172.xx.xx.172
  ssh_port: 32764
  port: 20160
  status_port: 20180
  deploy_dir: /tidb-deploy/tikv-20160
  data_dir: /tidb-data/tikv-20160
  arch: amd64
  os: linux

pd_servers:
  - host: 172.xx.xx.172
    ssh_port: 32764
    name: pd-1
    client_port: 2379
    peer_port: 2380
    deploy_dir: /tidb-deploy/pd-2379
    data_dir: /tidb-data/pd-2379

tidb_servers:
  - host: 172.xx.xx.172
    ssh_port: 32764
    port: 4000
    status_port: 10080
    deploy_dir: /tidb-deploy/tidb-4000

tiup cluster scale-out tidb scale-out.yaml
如果下线后pd仍然保留下线store的信息，需要手动删除该store上所有region的peer
/root/.tiup/components/ctl/v4.0.8/pd-ctl -i -u http://172.xx.xx.173:2379
region check down-peer
region store 4 --4为offline的kv节点的store id
operator add remove-peer 12 4 --移出store 4 上的region 12的一个副本
循环直到store 4上没有任何region的peer
此时store里面不再有offline的id为4的kv的信息
再次扩容即可
```

> 对于生产环境，几乎无法手动删除所有该故障store的region信息，可能需要借助脚本进行

### 6.回滚方案

#### 6.1 失败后降级回4.0.0版本

**不能降级**

![1](https://i.postimg.cc/76Mqp5vc/1.png)

#### 6.2 快照备份

华为云深信服提供的功能

快照有效，但需要评估快照生成和还原时间(9+1)

快照原理为生成快照后会持续记录变化，还原快照时将"回滚"这些变化，类似于undolog，猜测打上快照后大量更新操作会导致还原变慢，因此快照具有时效性

当天停止群集后再备(agent job33670，haproxy, spark，dm)

#### 6.3 有效的备份还原方案

**Dumpling:导出后测试使用tidb-lightling导入**

Dumpling导出正常输出

```html
dumpling -h 172.xx.xx.60 -P 4000 -u DBA_RC -t 16 -F 256MB -B EhaiPortals -B EhaiDsgDriver -p"" -o /data/my_database/
[2021/01/13 14:46:00.083 +08:00] [INFO] [main.go:195] ["dump data successfully, dumpling will exit now"]
```

Tidb-lightning配置文件

```html
[lightning]

# 转换数据的并发数，默认为逻辑 CPU 数量，不需要配置。
# 混合部署的情况下可以配置为逻辑 CPU 的 75% 大小。
# region-concurrency =

# 日志
level = "info"
file = "tidb-lightning.log"

[tikv-importer]
# backend 设置为 local 模式
backend = "local"
# 设置本地临时存储路径
sorted-kv-dir = "/tmp/sorted-kv-dir"

[mydumper]
# Mydumper 源数据目录。
data-source-dir = "/tmp/my_database"

[tidb]
# 目标集群的信息。tidb-server 的监听地址，填一个即可。
host = "172.xx.xx.172"
port = 4000
user = "root"
password = ""
# 表架构信息在从 TiDB 的“状态端口”获取。
status-port = 10080
# pd-server 的地址，填一个即可
pd-addr = "172.xx.xx.172:2379"
```

导出

```html
dumpling -h 172.xx.xx.60 -P 4000 -u DBA_RC -t 4 -r 500000 -B EHI_MessageEDM -p"" -o /data/my_database/
```

-t:并发导出的线程数
-r:将每张表分为r行为一个chunk导出

**-r和-t需要根据集群负载状态调整，保证导出一次性成功，dumpling无法进行断点续传**

导入

```html
nohup tidb-lightning -config tidb-lightning.toml > nohup.out &
```

正常日志输出

```html
tidb-lightning.log
[2021/01/13 09:17:15.223 +08:00] [INFO] [restore.go:304] ["the whole procedure completed"] [takeTime=57.654795852s] []
[2021/01/13 09:17:15.223 +08:00] [INFO] [main.go:95] ["tidb lightning exit"]

[2021/01/13 09:17:15.181 +08:00] [INFO] [pd.go:429] ["resume scheduler successful"] [scheduler=balance-region-scheduler]
[2021/01/13 09:17:15.181 +08:00] [INFO] [pd.go:520] ["restoring config"] [config="{\"enable-cross-table-merge\":\"false\",\"enable-debug-metrics\":\"false\",\"enable-location-replacement\":...
```

若tidb-lighting在导入时崩溃，需要从导入模式切回普通模式

```html
[2021/01/13 15:56:40.906 +08:00] [INFO] [sst_service.rs:109] ["switch mode"] [mode=Import]
tidb-lightning-ctl --switch-mode=normal
```

若tidb-lighting多次导入时报错，需要清理check point等相关信息

```html
tidb-lightning-ctl -tidb-host 172.xx.xx.172 --checkpoint-error-destroy=all
tidb-lightning-ctl -tidb-host 172.xx.xx.172 --checkpoint-remove=all
删除/tmp/tidb_lightning_checkpoint.pb

针对特定表的处理:
tidb-lightning-ctl --checkpoint-error-destroy='`schema`.`table`'
```

[详情](https://docs.pingcap.com/zh/tidb/stable/tidb-lightning-checkpoints#%E6%96%AD%E7%82%B9%E7%BB%AD%E4%BC%A0%E7%9A%84%E6%8E%A7%E5%88%B6)

**结论：升级前对群集所有节点做一个快照备份+特定（全部）DB的dumpling导出备份**

