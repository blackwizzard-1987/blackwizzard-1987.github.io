---
layout:     post
title:      基于Dify的问数应用设计开发小结
subtitle:
date:       2025-08-11
author:     RC
header-img:
catalog: true
tags:
    - 大模型
    - Dify
    - 自然语言处理
---


### 背景

目前，各行各业随着大模型普及，都纷纷建立了各种智能问答、智能客服、知识检索等应用，通过知识库检索、Agent化等方法，实现了**与AI自然语言对话完成特定场景任务**的目标。

在数据分析方面，如果我们能够通过和大模型对话的形式，完成数据查询和统计分析，将极大的**降低使用门槛**，使非专业的技术人员、业务人员都能够**自主查询和分析数据**。

出于这个理念，经过近两个月的努力，我们也是通过Dify成功迭代出一个自然语言问数的应用，虽然还谈不上商业化价值，但过程中确实学到了很多东西。本文对此进行简单介绍。

### 难点在哪

难点就三个，**准不准？快不快？连续不连续？**

首先我们要明白自然语言问数的原理，其实就是NL2SQL，即**自然语言转换为SQL查询语句**，这一点是肯定的，因为海量的数据不可能全部"喂"给大模型，而传统的智能问答系统使用**知识库召回片段完成定向检索的模式**，在问数这个场景下，**效果也不好**（后面会提到），所以只能通过写SQL完成查询，进而分析，主要流程为（简化版本）：

![1](https://i.postimg.cc/y6Fg4kZd/image.png)

可能这个时候有人就会说了，这个简单，就是**直接把数据库的表结构喂给大模型，然后直接提问**就行了。

确实，对于一个业务场景比较简单的系统，其表结构单一，没有歧义，不会引起AI幻觉，那确实可以直接省略其他优化，很直白的用表结构投喂，准确率就达到了。但问题在于，复杂场景下，一个应用要负责查询多个领域/业务条线的数据，那么就**需要问题分类，就需要提示词工程、schema增强去压缩有用的信息，需要约束AI的生成结果，避免幻觉**等等，而为了解决“快”的问题，你又要考虑**简单问题的模式化处理，历史问题的复用**，为了解决“连续回答”的问题，你又要**引入记忆池，对用户意图进行检测**...总之，在问数的场景下，怎么**提升准确度和速度**都是没错的，这也是整个应用优化的方向。

> 很直观的一点就是，从最初的demo（能用）到最终版本（好用），Dify的编排节点从一开始的几个增加了几十个，更别提各个代码执行节点背后的程序支撑，复杂程度呈几何级升

> 很棒的一点是，在我们的场景下，Dify的产品设计已经能够满足（除了官方说后面修复的发布后预览网页模式下，部分全局变量不生效的bug），当然，字节的扣子听说也很不错，但还没时间尝试。

### 探索过程

整个应用的探索过程称得上是摸着石头过河，毕竟没有相关设计经验，大模型相关的很多概念也是一知半解，只能依靠网上资料和官网的文档（deepseek API文档和Dify产品文档）慢慢摸索。

应用中很多的节点的程序编写以及架构和组件，都是和大模型多轮对话慢慢添加的，外部的需求、测试和反馈主要来自产品经理，总体符合测试，反馈，优化的流程，主要分为3个阶段：

![1](https://i.postimg.cc/JzX2PrQr/xiang-mu-gui-hua.png)

#### 起步阶段：准不准

这个阶段主要是满足问数应用最基本也是最核心的要求——**能够准确根据用户问题回答对应统计指标**。满足基本条件后，在实际场景下，我们还要考虑应用涵盖的**场景交叉**的问题、最终回答后的**用户引导**、**图形生成要求**等等。

##### 1、问题分类器

问题分类器主要保证了**应用涵盖的不同场景之间的隔离性**，因为每个场景的提示词和其他组件都是不相同的，按照dify的workflow设计，**问题分类这个组件**搭配合适的提示词，使用一般的LLM（如qwen-3.32b快速模式）即可比较准确的进行分类，保证用户问题->后续流程->场景之间的适配，同时，也可以直接在一开始就**拒绝回答通用性、常识性或者非统计性的不相干问题**，节约时间。

##### 2、提示词工程和schema增强描述

首先，我们需要在LLM处理生成SQL节点的system提示词中**明确模型的角色和任务**，并且给出**核心原则**（比如按照要求，参考表结构和理解用户问题，生成SQL），核心原则不宜过多，主要是帮助模型理解任务。

接下来，表结构对于整个应用来说重要性不言而喻，怎么让LLM更好的理解每个场景下面涉及的表是一个很基础但又很重要的问题。因为一般的关系型数据库的表结构，都是**大量离散信息的整合**，不适用于CoT(**Chain of Thoughts**)形式的框架，也不适合创造性内容，**对结果的准确性要求极高**，这也是自然语言问数的一个难点。

经过尝试，我们最终使用了**markdown格式**的改写版表结构作为LLM的提示词输入，主要包含了**表名、表描述、字段名、字段类型、字段空值与否、字段注释、字段示例值、主键和业务用途**。

需要注意的是，在梳理表结构后，需要结合实际业务，对某些**歧义性表结构内容进行约束**，对该**场景下面的通用内容进行约定**，在一般的场景中，涉及0-10张表的表结构，每张表约10-20字段基本没有大的问题。在关联表场景中，需要**追加关联表之间的关联关系描述**，包含关联的表和关联的字段信息。

这里也可以看出，**合理的表结构设计**，或者说**设计专门用于LLM使用的宽表**等，对于LLM的理解和降噪，提高准确性是很有帮助的。

同时，我们需要**对LLM的输出结果进行规范处理**，包含我们期望输出的内容（如json格式的SQL对），以及不希望模型输出的内容（如禁止输出其他内容，只需要SQL）。

另外，还需要在提示词末尾**给出1-2个用户问题和对应SQL的例子（即Few-Shot）**，也可以提高模型输出的准确率。

##### 3、决策引擎和图形生成

决策引擎是问题分类器进入分支后的又一个分支起点，主要用于**判断用户问题的两个维度：模糊性和复杂性**，并按照评估分数进入不同的后续分支处理。本质上是为了增加应用的泛用性和易用性。通过对用户问题的两个维度打分，可以更精细地处理不同类型问题，在准确度和效率上面都有所提高。

对于用户的**模糊查询**，比如：“广告投诉”、“噪音问题投诉”、“6月份群众投诉”等，这些**问题没有特别明确的业务维度或者时间周期**，所以，需要LLM根据我们约定的时间，作为统计周期，然后将相关字段作为业务维度，查询出所有的明细信息，然后**将这些信息整合起来形成统计摘要，最后反馈给用户**。这样做的好处是能够让不太懂业务或者分析的用户问题也能够找到他们想了解的方向，通过总览->引导问题->具体指标的路径，完成他们的数据分析探索。

对于用户的**简单查询**，比如“按类别统计本季度群众诉求前十”、“工单号23060003目前状态”、“上月群众诉求中投诉排名前五的部门”等，这些问题的业务维度非常明显且固定，也包含了明显的筛选条件或者计算口径及统计周期，对于这类**模式化的问题**，我们完全可以“枚举”可能的各种pattern，然后直接用**程序生成SQL**，极快又准，唯一的问题就是每个生成器都要定制化开发，并且不断地通过业务反哺完善。

剩下来的最后一类**复杂问题**，比如“成华区历史工单中环境卫生与市容秩序的数量比值？”、“预测下个季度的投诉趋势”、“市民最关心的问题是什么，有几个？分别是什么内容，哪个部门负责的”、“用折线图表示7月以来的投诉趋势”等，这些问题要么涉及到了**较复杂的统计口径**，比如环比、等比、比值，或者是**预测**类型，或者是**多个问题组合**，或者是**有图形生成要求**等等，我们都将他们划为复杂问题，必须借助LLM生成SQL。这里LLM的SQL准确度很依赖与第二个步骤中提示词工程的效果，我们也会发现即使temperature设置为0，模型依然会偶尔写错SQL（这里需要用**SQL校验器校验语法错误**，避免一部分问题，**AI幻觉依然不可避免**，比如无中生有某个不存在的字段）。但总体上，**LLM对于复杂问题的处理还是超乎想象，基本能够满足用户需求**。

图形生成要求我们通过Few-Shot能够轻松解决，只要给出一段**标准的Echarts Python代码**，模型就会根据实际图形要求生成各种图形，包括常见的**柱状图、折线图、饼图，以及雷达图、漏斗图**等等，不需要通过dify的Echarts插件进行标准化处理，枚举+分支太麻烦（虽然一开始我们就是这样做的）。

通过对用户问题的细分，不同路径的分支来具体处理不同类型和复杂度的问题，虽然应用整体框架更加复杂，**减少了可移植性**，但**易用性和性能会提高很多**，还是推荐做的。

这里的决策引擎也涉及到了第二个研究阶段解决“快不快“问题的办法，后面会继续介绍处理简单问题的程序规则生成器。

##### 4、用户引导和结果生成约束

#### 优化阶段：快不快

#### 深入阶段：连续不连续

### 实际效果概览

### 总结

### 参考





